# SARSA Snake Game Prompts

## Business Problem Statement
How can the SARSA algorithm be effectively utilized to train a snake agent to maximize rewards in the Snake Game, learning to avoid collisions while eating apples?

---

## Prompts

### 1. Basics of SARSA
1. What does SARSA stand for, and what are its core components?
2. How does SARSA handle on-policy updates, and why is it important in this context?
3. How does the Q-table represent the state-action space in SARSA?

### 2. Exploration vs. Exploitation
4. What is the purpose of the epsilon-greedy strategy in SARSA?
5. How can the epsilon value be adjusted over time to balance exploration and exploitation?
6. What are the consequences of choosing a high or low epsilon value for SARSA?

### 3. Hyperparameters
7. How do the learning rate and discount factor affect the SARSA algorithm's performance?
8. How would modifying the reward function (e.g., penalizing collisions) influence the snake's strategy?

### 4. Algorithm Performance
9. How does SARSA's on-policy nature impact its performance in stochastic environments?
10. How can you track the convergence of SARSA during training?

### 5. Advanced Considerations
11. How could you optimize SARSA for a larger or more complex Snake Game grid?
12. What are the computational trade-offs of using SARSA versus neural network-based approaches?
13. How does SARSA adapt when the game introduces dynamic obstacles?
14. Can SARSA handle continuous state spaces? If not, how might you extend it to do so?
15. How would you visualize and debug the agent's learning progress during training?
