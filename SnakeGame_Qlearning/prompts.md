# Q-Learning Snake Game Prompts

## Business Problem Statement
How can Q-learning be applied to train an agent to play the Snake Game by learning an optimal policy for maximizing rewards and minimizing collisions?

---

## Prompts

### 1. Basics of Q-Learning
1. What does the Q-value represent in Q-learning?
2. How does Q-learning differ from SARSA in terms of policy updates?
3. What role does the Q-table play in this implementation?

### 2. Hyperparameters and Reward Design
4. How do the learning rate and discount factor influence Q-learning’s ability to converge?
5. Why is the reward function critical in shaping the snake’s behavior? Provide examples of good and bad reward functions.
6. How would you modify the reward structure to encourage faster snake growth?

### 3. Exploration vs. Exploitation
7. How does the epsilon-greedy strategy balance exploration and exploitation in Q-learning?
8. What are the benefits of decaying the epsilon value over time?

### 4. Algorithm Performance
9. How does the size of the grid affect the Q-learning algorithm’s performance?
10. How can you measure the training progress and effectiveness of the Q-learning agent?

### 5. Advanced Concepts
11. How can Q-learning be extended to handle continuous state or action spaces?
12. What are the limitations of Q-learning when applied to environments with very large state spaces?
13. What optimizations (e.g., reward shaping or state abstraction) could improve the performance of Q-learning?
14. How would you adapt Q-learning for environments with dynamic or moving obstacles?
15. What visualization techniques can be used to debug the Q-learning agent?
